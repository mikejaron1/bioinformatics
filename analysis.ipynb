{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold, cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"/Users/mjaron/Google Drive/QMSS/translational_bioinformatics/Research Project/Data/\"\n",
    "variables = pd.read_csv(directory+'variables_master.csv')\n",
    "independent_variables = variables[variables['ind_dependent'] == 'independent']\n",
    "dependent_variables = variables[variables['ind_dependent'] == 'dependent']\n",
    "\n",
    "## creating a basic dictionary for lookup\n",
    "var_dict = {}\n",
    "for count, name in enumerate(variables['name']): \n",
    "    var_dict[str(name)] = {'label':variables['label'][count],\n",
    "                               'file': variables['file'][count],\n",
    "                               'year':variables['years'][count],\n",
    "                                'var_type': 'na',  ##could add type, i.e. range of values, code, binary, etc\n",
    "                                'ind_dep': variables['ind_dependent'][count]}\n",
    "\n",
    "## using this to find the exact variables for each file\n",
    "file_var_dict = {}\n",
    "for count, file_ in enumerate(variables['file']):\n",
    "    temp_df = variables[variables['file'] == file_]\n",
    "    var_list = temp_df['name']\n",
    "    year_list = temp_df['years']\n",
    "    file_var_dict[file_] = {'variables':list(var_list), 'year':list(year_list)}\n",
    "\n",
    "\n",
    "## getting actual data and reading into dict                  \n",
    "data_dict = {}\n",
    "for file_ in os.listdir(directory+'Demographic/'):\n",
    "    if '.XPT' in file_:\n",
    "        temp_df = pd.read_sas(directory+'Demographic/'+file_)\n",
    "        data_dict[file_[:-4]] = temp_df\n",
    "\n",
    "        \n",
    "file_abr = list(set(variables['file']))\n",
    "# print data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a filtered data dictionary where now it narrows down to varialbes I only car about plus adding in year\n",
    "## Gets entire df and just select columns needed, then adds in the year for each row.\n",
    "\n",
    "filt_data_dict = {}\n",
    "for name in data_dict.keys():\n",
    "    for abr in file_abr:\n",
    "        if abr in name:\n",
    "            temp_df = pd.DataFrame()\n",
    "            columns = file_var_dict[abr]['variables']\n",
    "            for i in columns:\n",
    "#                 print i\n",
    "                ## hard coded in this as the variable changes name slightly from 11'-14' but pretty much same\n",
    "                i1 = i\n",
    "                if i == 'ACD011A':\n",
    "                    i1 = 'ACD010A'\n",
    "                elif i == 'ACD011B':\n",
    "                    i1 = 'ACD010B'\n",
    "                elif i == 'DMDBORN4' or i == 'DMDBORN2':\n",
    "                    i1= 'DMDBORN'\n",
    "                elif i == 'HIQ011':\n",
    "                    i1= 'HID010'\n",
    "                elif i == 'OCD180':\n",
    "                    i1= 'OCQ180'\n",
    "                try:\n",
    "                    temp_df[i1] = data_dict[name][i]\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "#             temp_df[file_+'_year'] = [name for years in range(0,len(temp_df))] ## might not be necessary\n",
    "            temp_df['SEQN'] =  data_dict[name]['SEQN']\n",
    "            filt_data_dict[name] = temp_df\n",
    "# print filt_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RIDRETH1\n",
    "# print var_dict['RIDRETH1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## combine years for all same files and variables, since SEQN for each year seems to be unique\n",
    "\n",
    "## initiate dict\n",
    "combinded_var_dict ={}\n",
    "for i in filt_data_dict.keys():\n",
    "    for abr in file_abr:\n",
    "        if abr in i:\n",
    "            combinded_var_dict[abr] = pd.DataFrame()\n",
    "\n",
    "## combine all years together for each variable\n",
    "dep_list = list(dependent_variables['file'])\n",
    "for i in filt_data_dict.keys():\n",
    "    for abr in file_abr:\n",
    "        if abr in i:\n",
    "            combinded_var_dict[abr] = combinded_var_dict[abr].append(filt_data_dict[i])\n",
    "            if abr in dep_list:\n",
    "                combinded_var_dict[abr] = combinded_var_dict[abr].dropna()\n",
    "            combinded_var_dict[abr].reset_index(drop=True, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean(col, max_, max_val=np.nan, min_=0, min_val=np.nan, keep_na=True, \\\n",
    "              remove=False, rem_var=99, rem_rep_var=np.nan):\n",
    "    '''\n",
    "    input:\n",
    "        column\n",
    "    arguments:\n",
    "        lots of different ways to clean the column, with mostly preset info\n",
    "    output:\n",
    "        column\n",
    "    '''\n",
    "    col.loc[(col > max_)] = max_val\n",
    "    col.loc[(col < min_)] = min_val\n",
    "    if keep_na == False:\n",
    "        col = col.fillna(value=0)  \n",
    "    if remove == True:\n",
    "        col.loc[(col == rem_var)] = rem_rep_var\n",
    "            \n",
    "    return col\n",
    "\n",
    "#         df['ACD010B'] = df['ACD010B'].where(df['ACD010B'] != 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print combinded_var_dict.keys()\n",
    "# print combinded_var_dict['ACQ']['ACD010B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now clean data based on each uniqe variable\n",
    "## look at each set of variables 1 by 1 and edit if needed\n",
    "\n",
    "'''\n",
    "*ACD01** = 1 or 0, make any # > 1 = na, na = 0\n",
    "\n",
    "*DMDBORN = 1 or 0, make any # > 1 = na\n",
    "DMDHHSIZ = 1-7, 7 is 7 or more\n",
    "*DMDEDUC2 & DMDEDUC3 prob should be combined \n",
    "*DMDEDUC2 = 1-5 good, 6 or more = na\n",
    "*DMDEDUC3 = 1-15 good, 16 or more = na\n",
    "DMDHHSZA = 0-3 good\n",
    "DMDHHSZB = 0-4, 4 is 4 or more\n",
    "DMDHHSZE = 0-3, 3 is 3 or more\n",
    "*DMDHSEDU = 1-5 good, 6 or more = na\n",
    "*DMDMARTL = 1-6 good, 6 or greater *** not linear, each val is diff, prob need to split up into indiv binary columns\n",
    "*INDFMIN2 = 1-15 (not 12 & 13), good\n",
    "*RIAGENDR = 1-2 good *** categorical, needs to be split up\n",
    "RIDAGEYR = 0-80 good\n",
    "\n",
    "*HID010 = 1 yes , 2 no, change to 1 yes , 0 no\n",
    "\n",
    "LBXIN = range, good \n",
    "\n",
    "*OCD150 = 1 is working, anything above basically not working make 0\n",
    "*OCQ180 = 1 to 133\n",
    "'''\n",
    "\n",
    "for i in combinded_var_dict:\n",
    "    if 'ACQ' in i:\n",
    "        df = combinded_var_dict[i]\n",
    "        df['ACD010A'] = clean(df['ACD010A'], max_= 1, max_val=0, keep_na=False)\n",
    "        df['ACD010B'] = clean(df['ACD010B'], max_= 7, max_val=1, keep_na=False)\n",
    "        combinded_var_dict[i] = df\n",
    "        \n",
    "    elif 'DEMO' in i:\n",
    "        df = combinded_var_dict[i]\n",
    "        df['DMDBORN'] = clean(df['DMDBORN'], max_= 1, max_val=0)\n",
    "        df['DMDEDUC2'] = clean(df['DMDEDUC2'], max_= 6)\n",
    "        df['DMDEDUC3'] = clean(df['DMDEDUC3'], max_= 16, remove=True, rem_var=14)\n",
    "        df['DMDHSEDU'] = clean(df['DMDHSEDU'], max_= 5)\n",
    "\n",
    "        ## split up below categorical variable\n",
    "        df['DMDMARTL'] = clean(df['DMDMARTL'], max_= 6, keep_na=False)\n",
    "        new_var_list = ['Missing', 'Married', 'Widowed', 'Divorced', 'Separated', 'Never_married', 'Living_with_partner']\n",
    "        for item in range(0,7):\n",
    "            ## create empty list of 0 with same len\n",
    "            df['DMDMARTL_'+new_var_list[item]] = [0]*len(df['DMDMARTL']) \n",
    "            ## find the index of each item = to the number\n",
    "            df['DMDMARTL_'+new_var_list[item]].loc[(df['DMDMARTL'] == item)] = 1\n",
    "        ## get rid of the original column now since it has been split up    \n",
    "        df.drop('DMDMARTL', axis=1, inplace=True)\n",
    "        \n",
    "#         new_var_list = ['Male', 'Female']\n",
    "#         for item in range(1,3):\n",
    "#             ## create empty list of 0 with same len\n",
    "#             df['RIAGENDR_'+new_var_list[item-1]] = [0]*len(df['RIAGENDR']) \n",
    "#             ## find the index of each item = to the number\n",
    "#             df['RIAGENDR_'+new_var_list[item-1]].loc[(df['RIAGENDR'] == item)] = 1\n",
    "#         ## get rid of the original column now since it has been split up    \n",
    "#         df.drop('RIAGENDR', axis=1, inplace=True)\n",
    "        \n",
    "        df['INDFMIN2'] = clean(df['INDFMIN2'], max_= 15, remove=True, rem_var=12)\n",
    "        df['INDFMIN2'].loc[(df['INDFMIN2'] == 13)] = np.nan\n",
    "        combinded_var_dict[i] = df\n",
    "        \n",
    "    elif 'HIQ' in i:\n",
    "        df = combinded_var_dict[i]\n",
    "        df['HID010'] = clean(df['HID010'], max_= 2, remove=True, rem_var=2, rem_rep_var=0)\n",
    "        combinded_var_dict[i] = df\n",
    "        \n",
    "    elif 'L10AM' in i:\n",
    "        continue\n",
    "        \n",
    "    elif 'OCQ' in i:\n",
    "        df = combinded_var_dict[i]\n",
    "        df['OCD150'] = clean(df['OCD150'], max_= 1, max_val=0)\n",
    "        df['OCQ180'] = clean(df['OCQ180'], max_= 133)\n",
    "        combinded_var_dict[i] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get kernuys cleaned data and at it all together, for independent variables\n",
    "new_master = pd.DataFrame() \n",
    "for files in os.listdir(directory+'cleaned_data/'):\n",
    "    if '.csv' in files:\n",
    "        temp_df = pd.read_csv(directory+'cleaned_data/'+files)\n",
    "        temp_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        temp_df.drop('year', axis=1, inplace=True)\n",
    "        new_master = new_master.append(temp_df)\n",
    "        new_master = new_master.reset_index(drop=True)\n",
    "\n",
    "new_master['SEQN'] = new_master['seqn']\n",
    "new_master.drop('seqn', axis=1, inplace=True)\n",
    "\n",
    "## when ready add to main df\n",
    "combinded_var_dict['other'] = new_master\n",
    "\n",
    "print new_master.columns.tolist()\n",
    "print new_master.shape\n",
    "print combinded_var_dict.keys()\n",
    "\n",
    "for i in new_master.columns.tolist():\n",
    "    print i\n",
    "    print new_master[i].describe()\n",
    "    print '\\n'\n",
    "\n",
    "'''\n",
    "'alq120q', 'alq130', duplicates\n",
    "bpq100b, maybe to telling\n",
    "imq030, maybe to telling, vaccine\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### get kernyus dependent variables\n",
    "dependent_var_clean = pd.DataFrame() \n",
    "for files in os.listdir(directory+'cleaned_data/dependent/'):\n",
    "    if '.csv' in files:\n",
    "        temp_df = pd.read_csv(directory+'cleaned_data/dependent/'+files)\n",
    "        temp_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        temp_df.drop('year', axis=1, inplace=True)\n",
    "        dependent_var_clean = dependent_var_clean.append(temp_df)\n",
    "        dependent_var_clean = dependent_var_clean.reset_index(drop=True)\n",
    "\n",
    "dependent_var_clean['SEQN'] = dependent_var_clean['seqn']\n",
    "dependent_var_clean.drop('seqn', axis=1, inplace=True)\n",
    "dependent_var_clean.drop('lbxinsi', axis=1, inplace=True)\n",
    "dependent_var_clean.drop('lbxin', axis=1, inplace=True)\n",
    "## add in my cleaned column\n",
    "print dependent_var_clean.shape\n",
    "dependent_var_clean = dependent_var_clean.merge(combinded_var_dict['L10AM'], on='SEQN', how='inner')\n",
    "dependent_var_clean = dependent_var_clean.merge(combinded_var_dict['BPX'], on='SEQN', how='inner')\n",
    "# temp_bp1 = dependent_var_clean\n",
    "# temp_bp1 = temp_bp1.merge(combinded_var_dict['BPX'], on='SEQN', how='inner')\n",
    "\n",
    "print combinded_var_dict['BPX'].shape, 'here'\n",
    "print combinded_var_dict['L10AM'].shape\n",
    "print dependent_var_clean.shape\n",
    "print dependent_var_clean.head()\n",
    "# print new_master.sort('SEQN', ascending=False).head()\n",
    "# print combinded_var_dict['L10AM'].sort('SEQN', ascending=False).head()\n",
    "## when ready add to main df\n",
    "# combinded_var_dict['other'] = new_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prob can merge on SEQN since each year seems to be unique, and only merge all independent variables\n",
    "master = combinded_var_dict['ACQ']\n",
    "for df_name in combinded_var_dict: \n",
    "    if 'ACQ' not in df_name and df_name not in dep_list:\n",
    "        master = master.merge(combinded_var_dict[df_name], on='SEQN', how='outer')\n",
    "print master.shape\n",
    "# print master.describe()\n",
    "# print master['DMDHHSZA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## merge each df with each dependent variable to now use for the model\n",
    "model_data_dict = {}\n",
    "for i in dependent_var_clean.columns.tolist():\n",
    "    if i != 'SEQN':\n",
    "        temp_dep_df = pd.DataFrame()\n",
    "#         if i == 'BPXSY1' or i == 'BPXDI1' or i == 'BPXPLS':\n",
    "#             temp_dep_df['SEQN'] = combinded_var_dict['BPX']['SEQN']\n",
    "#             temp_dep_df[i] = combinded_var_dict['BPX'][i]\n",
    "#             model_data_dict[i] = master.merge(temp_dep_df, on='SEQN', how='inner')\n",
    "#             print model_data_dict[i].shape\n",
    "#         else:\n",
    "#             temp_dep_df['SEQN'] = dependent_var_clean['SEQN']\n",
    "#             temp_dep_df[i] = dependent_var_clean[i]\n",
    "#             model_data_dict[i] = master.merge(temp_dep_df, on='SEQN', how='inner')\n",
    "#             print model_data_dict[i].shape\n",
    "        temp_dep_df['SEQN'] = dependent_var_clean['SEQN']\n",
    "        temp_dep_df[i] = dependent_var_clean[i]\n",
    "        model_data_dict[i] = master.merge(temp_dep_df, on='SEQN', how='inner')\n",
    "        print model_data_dict[i].shape\n",
    "\n",
    "# for i in dependent_variables['file']:\n",
    "#     print i\n",
    "#     var = combinded_var_dict[i].columns.tolist()\n",
    "#     print var\n",
    "#     model_data_dict[var[0]] = master.merge(combinded_var_dict[i], on='SEQN', how='inner')\n",
    "#     print model_data_dict[var[0]].shape\n",
    "print model_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_data_dict['lbdldl']['DMDMARTL_Widowed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get rid of outliers\n",
    "for i in model_data_dict:\n",
    "    df = model_data_dict[i]\n",
    "    for col in df.columns.tolist():\n",
    "        #keep only the ones that are within +3 to -3 standard deviations in the column\n",
    "        df[col].loc[np.abs(df[col]-df[col].mean()) > (3*df[col].std())] = np.nan\n",
    "#         df = df[np.abs(df[col]-df[col].mean())<=(3*df[col].std())] \n",
    "    model_data_dict[i] = df\n",
    "#     print df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## fill nan, using mean of entire column for now\n",
    "\n",
    "for i in model_data_dict:\n",
    "    print i\n",
    "    col_length = []\n",
    "    df = model_data_dict[i]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "#     imp = Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=0, copy=True)\n",
    "#     imp.fit_transform(X)\n",
    "    for col in model_data_dict[i]:\n",
    "        perc = float(len(df[col].dropna()))/len(df)\n",
    "        print col, len(df[col].dropna()), len(df), perc\n",
    "        na_val = np.mean(df[col])\n",
    "        \n",
    "        ## find ~2000 rows with the least na's\n",
    "#         row_count = df.count(axis=1)\n",
    "#         row_count.sort_values(ascending=False, inplace=True)\n",
    "#         row_count = row_count.reset_index()\n",
    "#         ## get only top 2000 rows\n",
    "#         row_index = list(row_count['index'][:4000])\n",
    "        \n",
    "        ## threshold to get rid of columns if missing to much data\n",
    "        if np.isnan(na_val) or perc <= .4:\n",
    "            print \"dropped \", col\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(na_val, inplace=True)\n",
    "        ## now after replacement and threshold keep only top 2000 rows with least NA's calculated abovee\n",
    "#     df = df.loc[row_index]\n",
    "#     df = df.sample(n=2000)\n",
    "        \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    model_data_dict[i] = df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp = pd.DataFrame({'a':[1,2,3,4,5,np.nan,6], 'b':[2,3,4,6,np.nan,np.nan,9]})\n",
    "# a = temp.count(axis=1)\n",
    "# print a.sort_values(ascending=False, inplace=True)\n",
    "# print a\n",
    "# b = a.reset_index()\n",
    "# k = list(b['index'][:3])\n",
    "# print k\n",
    "# print temp.loc[k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## find out which variables correlate with the dependent var\n",
    "\n",
    "## general features to remove for all\n",
    "# features_to_remove = ['ACD010B', 'DMDMARTL_Widowed', 'DMDMARTL_Divorced', 'DMDMARTL_Separated', \\\n",
    "#                       'DMDMARTL_Living_with_partner', 'DMDMARTL_Missing']\n",
    "\n",
    "print model_data_dict.keys()\n",
    "\n",
    "for i in model_data_dict:\n",
    "    print i\n",
    "#     try:\n",
    "#         model_data_dict[i] = model_data_dict[i].drop(features_to_remove, axis=1)\n",
    "#     except:\n",
    "#         pass\n",
    "    df = model_data_dict[i]\n",
    "    a = df.corr()\n",
    "#     print a\n",
    "#     b = a[i]\n",
    "#     print b.sort(ascending=True, kind='quicksort', na_position='last', inplace=False)\n",
    "    high = []\n",
    "    for col in df.columns.tolist():\n",
    "        b = a[col]\n",
    "        b = b.reset_index()\n",
    "        for count,x in enumerate(b[col]):\n",
    "            if abs(x) > .2 and abs(x) != 1:\n",
    "                high.append((col,b['index'][count],x))\n",
    "    print '*********************'\n",
    "    break\n",
    "print high\n",
    "# plt.scatter(model_data_dict['lbdldl']['RIAGENDR'],model_data_dict['lbdldl']['whd120'])\n",
    "# a = model_data_dict['lbdldl']['lbdldl']\n",
    "# b = model_data_dict['lbxsch']['lbxsch']\n",
    "# c = model_data_dict['lbxstr']['lbxstr']\n",
    "# print '\\n'\n",
    "# print np.corrcoef(a,b)\n",
    "# print np.corrcoef(a,c)\n",
    "# print np.corrcoef(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## correlations\n",
    "'''\n",
    "[('RIDAGEYR', 'whq150', 0.63273907024842002), ('bmxbmi', 'bmxwaist', 0.86370485543272124), \n",
    "('bmxbmi', 'whd020', 0.66453481733176267), ('bmxwaist', 'bmxbmi', 0.86370485543272124), \n",
    "('bmxwaist', 'whd020', 0.67986006498703533), ('whd020', 'bmxbmi', 0.66453481733176267), \n",
    "('whd020', 'bmxwaist', 0.67986006498703533), ('whd020', 'whd110', 0.56738619317411143), \n",
    "('whd020', 'whd120', 0.542019736144144), ('whd110', 'whd020', 0.56738619317411143), \n",
    "('whd110', 'whd120', 0.6215479165156752), ('whd120', 'whd020', 0.542019736144144), \n",
    "('whd120', 'whd110', 0.6215479165156752), ('whq150', 'RIDAGEYR', 0.63273907024842002)]\n",
    "'''\n",
    "## variables to remove\n",
    "## after the 3rd row is experimentation\n",
    "features_to_remove = ['DMDEDUC3', 'ACD010B', 'DMDMARTL_Widowed', 'DMDMARTL_Missing', \n",
    "                      'DMDMARTL_Divorced','DMDMARTL_Separated', 'DMDMARTL_Living_with_partner', 'hsq520', \n",
    "                      'bmxwaist', 'whd020', 'whq150', 'whd110', ## > 50% correlation with other variables\n",
    "                     'DMDMARTL_Never_married', 'DMDHSEDU', 'DMDHHSIZ', 'DMDBORN'  ## > 40% correlation\n",
    "                     ]\n",
    "\n",
    "print model_data_dict.keys()\n",
    "\n",
    "for i in model_data_dict:\n",
    "    print i\n",
    "    print model_data_dict[i].shape\n",
    "#     print model_data_dict[i].columns.tolist()\n",
    "    for var in features_to_remove:\n",
    "        try:\n",
    "            model_data_dict[i] = model_data_dict[i].drop(var, axis=1)\n",
    "        except:\n",
    "#             print \"didnt drop \", var\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### turn into classification model, requires more knowledge of data\n",
    "'''\n",
    "http://www.healthline.com/health/high-cholesterol/levels-by-age#3\n",
    "lbdldl\n",
    "LDL\n",
    "Good: 130 mg/dL or lower  (0)\n",
    "Borderline: 130 to 159 mg/dL   (1)\n",
    "High: 160 mg/dL or higher   (2)\n",
    "\n",
    "lbxsch\n",
    "Total Cholesterol\n",
    "Good: 200 mg/dL or lower   (0)\n",
    "Borderline: 200 to 239 mg/dL   (1)\n",
    "High: 240 mg/dL or higher   (2)\n",
    "\n",
    "lbxstr (triglycerides)\n",
    "Triglycerides\n",
    "Good: 149 mg/dL or lower  (0)\n",
    "Borderline: 150 to 199 mg/dL   (1)\n",
    "High: 200 mg/dL or higher   (2)\n",
    "\n",
    "lbxsgl (glucose)\n",
    "\n",
    "\n",
    "lbxsca (calcium)\n",
    "\n",
    "\n",
    "LBXIN (Insulin)\n",
    "A study in Arizona found that women with a fasting insulin level around 8.0 had twice the risk of prediabetes as \n",
    "did women with a level around 5.0. Women with a fasting insulin of 25 or so had five times the risk of prediabetes.\n",
    "\n",
    "(if to the tenth then 6.5)\n",
    "2-6 good\n",
    "> 7 bad\n",
    "\n",
    "lbxsir (Iron)\n",
    "\n",
    "\n",
    "'BPXDI1' (diastolic)  'BPXSY1' (systolic)\n",
    "90 over 60 (90/60) or less: You may have low blood pressure\n",
    "More than 90 over 60 (90/60) and less than 120 over 80 (120/80): Your blood pressure reading is ideal and healthy\n",
    "More than 120 over 80 and less than 140 over 90 (120/80-140/90): You have a normal blood pressure reading but it is \n",
    "a little higher than it should be, and you should try to lower it\n",
    "140 over 90 (140/90) or higher (over a number of weeks): You may have high blood pressure (hypertension).\n",
    "Change your lifestyle - see your doctor or nurse and take any medicines they may give you. \n",
    "3 = high\n",
    "2 = pre-high\n",
    "1 = good\n",
    "0 = low\n",
    "\n",
    "'BPXPLS'  (pulse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "cat_list = ['lbdldl', 'lbxsch', 'lbxstr', 'BP', 'LBXIN']\n",
    "## copy data to new dict name\n",
    "\n",
    "var = 'LBXIN'\n",
    "if max(model_data_dict[var][var]) > 5:\n",
    "#     plt.hist(model_data_dict[var][var])\n",
    "    model_data_dict[var].reset_index(drop=True, inplace=True)\n",
    "    df = model_data_dict[var]\n",
    "    df[var].loc[df[var] <= 6.599] = 0\n",
    "    df[var].loc[df[var] >= 6.6] = 1\n",
    "    model_data_dict[var] = df\n",
    "    \n",
    "\n",
    "\n",
    "dia = 'BPXDI1'\n",
    "sys = 'BPXSY1'\n",
    "if max(model_data_dict['BPXDI1']['BPXDI1']) > 5:\n",
    "    model_data_dict[dia].reset_index(drop=True, inplace=True)\n",
    "    model_data_dict[sys].reset_index(drop=True, inplace=True)\n",
    "    dia = model_data_dict[dia]\n",
    "    sys = model_data_dict[sys]\n",
    "    temp_sys = pd.DataFrame()\n",
    "    temp_sys['SEQN'] = sys['SEQN']\n",
    "    temp_sys['BPXSY1'] = sys['BPXSY1']\n",
    "    df = dia.merge(temp_sys, on='SEQN', how='inner')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    BP_cat = []\n",
    "    for d,s in zip(df['BPXDI1'],df['BPXSY1']):\n",
    "#         if s < 90 and d < 60:\n",
    "#             BP_cat.append(0)\n",
    "#         if 90 < s <= 120 and 60 < d <= 80:\n",
    "#             BP_cat.append(1)\n",
    "#         if 120 < s <= 140 and 80 < d <= 90: \n",
    "#             BP_cat.append(2)\n",
    "#         if 140 < s or 90 < d:\n",
    "#             BP_cat.append(3)\n",
    "        if s > 140 or d > 90:\n",
    "            BP_cat.append(3)\n",
    "        elif s <= 90 and d <= 60:\n",
    "            BP_cat.append(0)\n",
    "        elif s <= 140:\n",
    "            if d > 80 and d <= 90:\n",
    "                BP_cat.append(2)\n",
    "            elif s > 120:\n",
    "                BP_cat.append(2)\n",
    "            elif s <= 120:\n",
    "                if d > 60 and d <= 80:\n",
    "                    BP_cat.append(1)\n",
    "                elif s > 90:\n",
    "                    BP_cat.append(1)\n",
    "            else:\n",
    "                BP_cat.append(0)\n",
    "    df.drop(['BPXSY1','BPXDI1'], axis=1, inplace=True)\n",
    "    df['BP'] = BP_cat\n",
    "    model_data_dict['BP'] = df\n",
    "\n",
    "\n",
    "var = 'lbdldl'\n",
    "if max(model_data_dict[var][var]) > 5:\n",
    "    model_data_dict[var].reset_index(drop=True, inplace=True)\n",
    "    df = model_data_dict[var]\n",
    "    df[var].loc[df[var] <= 129] = 0\n",
    "    df[var].loc[(df[var] >= 130) & (df[var] <= 159)] = 1\n",
    "    df[var].loc[df[var] >= 160] = 2\n",
    "\n",
    "    model_data_dict[var] = df\n",
    "    \n",
    "    \n",
    "var = 'lbxsch'\n",
    "if max(model_data_dict[var][var]) > 5:\n",
    "    model_data_dict[var].reset_index(drop=True, inplace=True)\n",
    "    df = model_data_dict[var]\n",
    "    df[var].loc[df[var] <= 200] = 0\n",
    "    df[var].loc[(df[var] >= 201) & (df[var] <= 239)] = 1\n",
    "    df[var].loc[df[var] >= 240] = 2\n",
    "\n",
    "    model_data_dict[var] = df\n",
    "    \n",
    "var = 'lbxstr'\n",
    "if max(model_data_dict[var][var]) > 5:\n",
    "    model_data_dict[var].reset_index(drop=True, inplace=True)\n",
    "    df = model_data_dict[var]\n",
    "    df[var].loc[df[var] <= 149] = 0\n",
    "    df[var].loc[(df[var] >= 150) & (df[var] <= 199)] = 1\n",
    "    df[var].loc[df[var] >= 200] = 2\n",
    "\n",
    "    model_data_dict[var] = df\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # print 'new'\n",
    "# print df.head()\n",
    "# print 'old'\n",
    "# print model_data_dict['lbdldl']['lbdldl'].head()\n",
    "# print model_data_dict.keys()\n",
    "# print model_data_dict['lbdldl_cat']['lbdldl'].head()\n",
    "# print df['lbdldl_cat'].head()\n",
    "# print '*****'\n",
    "# print model_data_dict['lbdldl']['lbdldl'].head()\n",
    "\n",
    "\n",
    "## svm regressor or linear regressor to check\n",
    "## look into outliers by lookign looking at scatterplots\n",
    "## look at correlations between features\n",
    "## look at correlations between X and Y, if 0 correlation take it out, easy to look at scatterplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regr_acc(X_validation, Y_validation, model):\n",
    "    # The mean squared error\n",
    "    mse = np.mean((model.predict(X_validation) - Y_validation) ** 2)\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mse)\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % model.score(X_validation, Y_validation))\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_data_dict[dependent]['LBXIN'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for dependent in model_data_dict:\n",
    "dependent = 'BP'\n",
    "feature_list = model_data_dict[dependent].drop([dependent,'SEQN'], axis=1).columns.tolist()\n",
    "Y = np.array(model_data_dict[dependent][dependent])\n",
    "X = np.array(model_data_dict[dependent].drop([dependent,'SEQN'], axis=1))\n",
    "drop = []\n",
    "# for i in range(len(feature_list)):\n",
    "#     print i\n",
    "validation_size = 0.40\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, \\\n",
    "                                                            test_size=validation_size, random_state=seed)\n",
    "Y_validation = Y_train\n",
    "X_validation = X_train\n",
    "\n",
    "if categories:\n",
    "    if dependent in cat_list:\n",
    "        print dependent\n",
    "        model = RandomForestClassifier(n_estimators=100, min_samples_split=2,min_samples_leaf=1,\\\n",
    "                                       n_jobs=-1, max_features='auto', random_state=seed)\n",
    "\n",
    "        score = cross_val_score(model, X_train, Y_train, cv=10)\n",
    "        print(\"Accuracy: %s (+/- %s)\" % (score.mean(), score.std()))\n",
    "        cv_predict = cross_val_predict(model, X_validation,Y_validation, cv=10)\n",
    "        print accuracy_score(Y_validation, cv_predict), \" CV HO\"\n",
    "        print pd.crosstab(Y_validation, cv_predict, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "        print(classification_report(Y_validation, cv_predict))\n",
    "        model.fit(X_train, Y_train) \n",
    "        impo_features = feature_importance(model, X, feature_list, dependent, show_ranking=False,\\\n",
    "                                           csv=True, show_plot=True)\n",
    "        roc_curve_plot(X,Y, dependent, RF_main_model) \n",
    "#         plot_learning_curve(model, dependent, X_train, Y_train, ylim=(0.5, 1.01), cv=10, n_jobs=4)\n",
    "\n",
    "\n",
    "#                 print impo_features\n",
    "#             drop.append(impo_features[-1])\n",
    "#             X = np.array(model_data_dict[dependent].drop([dependent,'SEQN'], axis=1))\n",
    "#             feature_list = model_data_dict[dependent].drop([dependent,'SEQN'], axis=1).columns.tolist()\n",
    "#             for item in drop:\n",
    "#                 X = np.array(model_data_dict[dependent].drop(item, axis=1))\n",
    "#                 feature_list = model_data_dict[dependent].drop(item, axis=1).columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## split up the data\n",
    "categories = True\n",
    "roc_scores = {}\n",
    "RF_scores = {}\n",
    "# var_dict = {}\n",
    "for dependent in model_data_dict:\n",
    "    print dependent\n",
    "    feature_list = model_data_dict[dependent].drop([dependent,'SEQN'], axis=1).columns.tolist()\n",
    "    temp_list = []\n",
    "    \n",
    "    Y = np.array(model_data_dict[dependent][dependent])\n",
    "    X = np.array(model_data_dict[dependent].drop([dependent,'SEQN'], axis=1))\n",
    "    \n",
    "    validation_size = 0.4\n",
    "    seed = 1\n",
    "    X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, \\\n",
    "                                                                test_size=validation_size, random_state=seed)\n",
    "#     Y_validation = Y_train\n",
    "#     X_validation = X_train\n",
    "    ### could do cross validation with all data\n",
    "    if categories:\n",
    "        if dependent in cat_list:\n",
    "            print dependent\n",
    "    #         print '\\n'\n",
    "            print \"categorical\"\n",
    "            model = RandomForestClassifier(n_estimators=100, min_samples_split=2,min_samples_leaf=1,\\\n",
    "                                           n_jobs=-1, max_features='auto', random_state=seed)\n",
    "            RF_main_model = model\n",
    "            model.fit(X_train, Y_train) \n",
    "            print model.score(X_train,Y_train), 'on training'\n",
    "            predicted = model.predict(X_validation)\n",
    "            ho_score = model.score(X_validation, Y_validation)\n",
    "            print ho_score, 'on hold out'\n",
    "            \n",
    "            score = cross_val_score(model, X_train, Y_train, cv=10)\n",
    "            print(\"Accuracy: %s (+/- %s)\" % (score.mean(), score.std()))\n",
    "            RF_scores[dependent] = [{'HO':ho_score, \"CV\":score}]\n",
    "#             print pd.crosstab(Y_validation, predicted, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "#             print 'cv'\n",
    "            cv_predict = cross_val_predict(model, X_validation,Y_validation, cv=10)\n",
    "            print accuracy_score(Y_validation, cv_predict), \" CV HO\"\n",
    "        \n",
    "#             print accuracy_score(Y_validation, cv_predict, normalize=False), \" CV HO, count\"\n",
    "#             print f1_score(Y_validation, cv_predict, average='weighted'), \"F1 score\"\n",
    "            \n",
    "#             print cross_val_score(model, X, Y, cv=10).mean(), \"all the data\"\n",
    "#             cv_predict_all = cross_val_predict(model, X,Y, cv=10)\n",
    "#             print pd.crosstab(Y, cv_predict_all, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "#             print 'end all test'\n",
    "            print pd.crosstab(Y_validation, cv_predict, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "            print(classification_report(Y_validation, cv_predict))\n",
    "            plot_learning_curve(model, dependent, X_train, Y_train, ylim=(0.5, 1.01), cv=10, n_jobs=4)\n",
    "#             roc_curve_plot(X,Y, dependent, RF_main_model) \n",
    "\n",
    "#             print temp_list\n",
    "            feature_importance(model, X, feature_list, dependent, show_ranking=True,\\\n",
    "                                                   csv=True, show_plot=True)\n",
    "\n",
    "\n",
    "#     else:\n",
    "#         print dependent\n",
    "#         print \"linear\"\n",
    "#         model = RandomForestRegressor()  \n",
    "#     #     model = PLSRegression(n_components=1)\n",
    "        \n",
    "#         model.fit(X_train, Y_train) \n",
    "#         print model.score(X_train,Y_train), \"R^2\"\n",
    "# #         prediction = model.predict(X_validation)\n",
    "#         mse = regr_acc(X_validation, Y_validation, model) ## my own function for mean sqrd error and variance\n",
    "#         score = cross_val_score(model, X_train, Y_train, cv=10, scoring='neg_mean_squared_error').mean()\n",
    "#         print score\n",
    "#         cv_predict = cross_val_predict(model, X_validation,Y_validation)\n",
    "#         mse = np.mean((cv_predict- Y_validation) ** 2)\n",
    "#         RF_scores[dependent] = [{'HO':mse, \"CV\":score}]\n",
    "#         feature_importance(model, X)\n",
    "\n",
    "#     print accuracy_score(prediction,Y_validation)\n",
    "\n",
    "#     print type(X), type(Y)\n",
    "#     # prepare models\n",
    "#     models = []\n",
    "#     models.append(('RF', RandomForestRegressor()))\n",
    "#     models.append(('PLS', PLSRegression(n_components=1)))\n",
    "#     models.append(('KR', KernelRidge(alpha=1.0)))\n",
    "#     models.append(('LR', LinearRegression()))\n",
    "    \n",
    "    \n",
    "    # evaluate each model in turn\n",
    "#             num_folds = 10\n",
    "#             num_instances = len(X_train)\n",
    "#             seed = 7\n",
    "#             scoring = 'accuracy'\n",
    "#             models = []\n",
    "#             models.append(('LR', LogisticRegression()))\n",
    "# #             models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# #             models.append(('KNN', KNeighborsClassifier()))\n",
    "# #             models.append(('CART', DecisionTreeClassifier()))\n",
    "# #             models.append(('NB', GaussianNB()))\n",
    "# #             models.append(('SVM', SVC()))\n",
    "#             # evaluate each model in turn\n",
    "#             results = []\n",
    "#             names = []\n",
    "#             for name, model in models:\n",
    "#                 kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "#                 cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "#                 results.append(cv_results)\n",
    "#                 names.append(name)\n",
    "#                 msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#                 print(msg)\n",
    "#                 print sigmoid( dot([val1, val2], lr.coef_) + lr.intercept_ ) \n",
    "#                 print model.fit(X_train,Y_train).predict_proba(X_validation)\n",
    "#                 pca = PCA(n_components=2)\n",
    "#                 cls = LogisticRegression() \n",
    "#                 pipe = Pipeline([('pca', pca), ('logistic', clf)])\n",
    "#                 pipe.fit(X_train, Y_train)\n",
    "#                 predictions = pipe.predict(X_validation)\n",
    "#                 print accuracy_score(Y_validation, predictions)\n",
    "#                 print pipe.score(X_validation, Y_validation)\n",
    "                \n",
    "    # boxplot algorithm comparison\n",
    "#     fig = plt.figure()\n",
    "#     fig.suptitle('Algorithm Comparison')\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     plt.boxplot(results)\n",
    "#     ax.set_xticklabels(names)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print RF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Default\n",
    "lbdldl\n",
    "categorical\n",
    "0.718861209964 on hold out\n",
    "Accuracy: 0.71 (+/- 0.00)\n",
    "LR: 0.735909 (0.007158)\n",
    "LDA: 0.736014 (0.006932)\n",
    "KNN: 0.704350 (0.007298)\n",
    "lbxsch\n",
    "categorical\n",
    "0.637429348964 on hold out\n",
    "Accuracy: 0.63 (+/- 0.00)\n",
    "LR: 0.661381 (0.008607)\n",
    "LDA: 0.661329 (0.007788)\n",
    "KNN: 0.630607 (0.007509)\n",
    "lbxstr\n",
    "categorical\n",
    "0.776219384551 on hold out\n",
    "Accuracy: 0.78 (+/- 0.00)\n",
    "LR: 0.798972 (0.009357)\n",
    "LDA: 0.798658 (0.009168)\n",
    "KNN: 0.780236 (0.008511)\n",
    "\n",
    "min_samples_leaf=50\n",
    "lbdldl\n",
    "categorical\n",
    "0.746284278836 on hold out\n",
    "Accuracy: 0.74 (+/- 0.00)\n",
    "lbxsch\n",
    "categorical\n",
    "0.674481892401 on hold out\n",
    "Accuracy: 0.66 (+/- 0.00)\n",
    "lbxstr\n",
    "categorical\n",
    "0.790663596399 on hold out\n",
    "Accuracy: 0.80 (+/- 0.00)\n",
    "\n",
    "100 estimators\n",
    "lbdldl\n",
    "categorical\n",
    "0.737910822692 on hold out\n",
    "Accuracy: 0.73 (+/- 0.00)\n",
    "lbxsch\n",
    "categorical\n",
    "0.653757588445 on hold out\n",
    "Accuracy: 0.65 (+/- 0.00)\n",
    "lbxstr\n",
    "categorical\n",
    "0.785639522713 on hold out\n",
    "Accuracy: 0.79 (+/- 0.00)\n",
    "\n",
    "Default\n",
    "lbdldl\n",
    "Predicted   0.0  1.0  2.0   All\n",
    "True                           \n",
    "0.0        3492   61   12  3565\n",
    "1.0         805   28    3   836\n",
    "2.0         362   14    0   376\n",
    "All        4659  103   15  4777\n",
    "lbxsch\n",
    "Predicted   0.0  1.0  2.0   All\n",
    "True                           \n",
    "0.0        3024  175   23  3222\n",
    "1.0         946  123   21  1090\n",
    "2.0         394   65    6   465\n",
    "All        4364  363   50  4777\n",
    "lbxstr\n",
    "Predicted   0.0  1.0  2.0   All\n",
    "True                           \n",
    "0.0        3743   21   13  3777\n",
    "1.0         556    6    5   567\n",
    "2.0         427    4    2   433\n",
    "All        4726   31   20  4777\n",
    "\n",
    "\n",
    "min leaf 100\n",
    "lbdldl\n",
    "categorical\n",
    "Predicted   0.0   All\n",
    "True                 \n",
    "0.0        3565  3565\n",
    "1.0         836   836\n",
    "2.0         376   376\n",
    "All        4777  4777\n",
    "lbxsch\n",
    "Predicted   0.0   All\n",
    "True                 \n",
    "0.0        3222  3222\n",
    "1.0        1090  1090\n",
    "2.0         465   465\n",
    "All        4777  4777\n",
    "lbxstr\n",
    "Predicted   0.0   All\n",
    "True                 \n",
    "0.0        3777  3777\n",
    "1.0         567   567\n",
    "2.0         433   433\n",
    "All        4777  4777\n",
    "\n",
    "\n",
    "Notes:\n",
    "- First istead of getting rid of outliers, I made them 'na', then afterwords computed the mean replacement\n",
    "- Seems Logit might be best\n",
    "- Tried manipulating RF model, with leaf sample size and estimators and other things\n",
    "- tried PCA and logit, worse off\n",
    "- looked at learning curve and decided to reduce the amount of NA's by only keeping only 2000 of the least NA rows\n",
    "  -- get rid of outliers\n",
    "  -- get rid of columns with < 40% of data\n",
    "  -- do mean replacement of NA's on all columns\n",
    "  \n",
    "- since we dont need a lot of data changed the training size to 40% in order to have a more robust predictions score\n",
    "-removed any features with > 50% correlations\n",
    "\n",
    "- very high correlation between ldl and totchl 0.76695075 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = np.logspace(-6, -1, 5)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RF_main_model, X, Y, param_name=\"gamma\", param_range=param_range,\n",
    "    cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RF\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ROC curve\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "def roc_curve_plot(X,Y, dependent, model):\n",
    "    # Binarize the output\n",
    "    y = label_binarize(Y, classes=[0, 1, 2])\n",
    "    if dependent == 'BP':\n",
    "        y = label_binarize(Y, classes=[0, 1, 2, 3])\n",
    "    n_classes = y.shape[1]\n",
    "\n",
    "    # shuffle and split training and test sets\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=.8,\n",
    "                                                        random_state=0)\n",
    "\n",
    "#     model = RandomForestClassifier()\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    y_score = classifier.fit(X_train, Y_train).predict_proba(X_validation)\n",
    "\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y_validation[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_validation.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "    \n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    if dependent == 'BP':\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class  -'+dependent)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# roc_curve_plot(X,Y, dependent)   \n",
    "\n",
    "'''\n",
    "'micro':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    print \"slope \", (test_scores_mean[0] - test_scores_mean[-1]) / (train_sizes[0] - train_sizes[-1])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "#     return plt\n",
    "\n",
    "# title=dependent\n",
    "# cv = ShuffleSplit(n_splits=10, test_size=0.4, random_state=0)\n",
    "# plot_learning_curve(RF_main_model, title, X_train, Y_train, ylim=(0, 1.01), cv=cv, n_jobs=4)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot12(y, predicted):\n",
    "    ax = fig.add_subplots()\n",
    "    ax.scatter(y, predicted)\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dependent = 'lbxsgl'\n",
    "feature_list = model_data_dict[dependent].drop([dependent,'SEQN'], axis=1).columns.tolist()\n",
    "Y = np.array(model_data_dict[dependent][dependent])\n",
    "X = np.array(model_data_dict[dependent].drop([dependent,'SEQN'], axis=1))\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, \\\n",
    "                                                                test_size=validation_size, random_state=seed)\n",
    "y = Y_validation\n",
    "model.fit(X_train, Y_train) \n",
    "predicted = model.predict(X_validation)\n",
    "\n",
    "print \"hold out\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "model = RandomForestRegressor() \n",
    "# y = Y_train\n",
    "# predicted = cross_val_predict(model, X_train, Y_train, cv=10)\n",
    "predicted = cross_val_predict(model, X_validation, Y_validation, cv=10)\n",
    "\n",
    "\n",
    "print \"cross val\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_importance(model, X, feature_list, dependent, show_ranking=True, csv=False, show_plot=True):\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "#     print feature_list[1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    temp = []\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for f in range(X.shape[1]):\n",
    "        if show_ranking:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "        temp.append(importances[indices[f]])\n",
    "        temp1.append(indices[f])\n",
    "        temp2.append(feature_list[indices[f]])\n",
    "            \n",
    "    if csv:\n",
    "        df = pd.DataFrame()\n",
    "        df[dependent+'_importance_rank'] = range(1,len(temp1)+1)\n",
    "        df['feature'] = temp1\n",
    "        df['important_score'] = temp\n",
    "        df['name'] = temp2\n",
    "        \n",
    "        df.to_csv(dependent+'_feature_importance.csv', index=False)\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(15,5))\n",
    "#     ax = fig.add_subplot()\n",
    "#     plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "           color=\"orange\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return temp2\n",
    "# feature_importance(model, X, feature_list, dependent, ranking=True, csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Test options and evaluation metric\n",
    "# num_folds = 10 # 5 or 10 does not really change\n",
    "# num_instances = len(X_train)\n",
    "# seed = 7\n",
    "# scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PLS regression\n",
    "PLS_scores = {}\n",
    "for dependent in model_data_dict:\n",
    "    print dependent\n",
    "    if 'cat' in dependent:\n",
    "        categories = True\n",
    "        dependent = dependent[:dependent.find('_cat')]\n",
    "    else:\n",
    "        categories = False\n",
    "    feature_list = model_data_dict[dependent].drop([dependent,'SEQN'], axis=1).columns.tolist()\n",
    "#     print model_data_dict[dependent][dependent].head()\n",
    "    Y = np.array(model_data_dict[dependent][dependent])\n",
    "    X = np.array(model_data_dict[dependent].drop([dependent,'SEQN'], axis=1))\n",
    "    \n",
    "    validation_size = 0.20\n",
    "    seed = 7\n",
    "    X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, \\\n",
    "                                                                test_size=validation_size, random_state=seed)\n",
    "    pls2 = PLSRegression(n_components=1)\n",
    "    # pls2.fit(X_train, Y_train)\n",
    "    # PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,\n",
    "    #         tol=1e-06)\n",
    "    # Y_pred = pls2.predict(X_validation)\n",
    "    x,y = pls2.fit_transform(X_train, Y_train)\n",
    "    # pls2.fit(x, y)\n",
    "#     print pls2.get_params(deep=True)\n",
    "    mse = regr_acc(X_validation, Y_validation, pls2) \n",
    "    score = cross_val_score(pls2, X_train, Y_train, cv=10, scoring='neg_mean_squared_error').mean()\n",
    "    print score\n",
    "    cv_predict = cross_val_predict(pls2, X_validation,Y_validation)\n",
    "    mse = np.mean((cv_predict- Y_validation) ** 2)\n",
    "    PLS_scores[dependent] = [{'HO':mse, \"CV\":score}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'rf'\n",
    "print RF_scores\n",
    "print 'pls'\n",
    "print PLS_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## start with the models\n",
    "regr = LinearRegression(fit_intercept=True, normalize=False)\n",
    "# kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "\n",
    "# cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "predictions = regr.predict(X_validation)\n",
    "\n",
    "m = regr.coef_\n",
    "b = regr.intercept_\n",
    "# print \"formula: y = {0}x + {1}\".format(m,b)\n",
    "\n",
    "# print(accuracy_score(Y_validation, predictions))\n",
    "## row is the thing it is actually in, in this case lots of things are thought of as 4\n",
    "# print(confusion_matrix(Y_validation, predictions))\n",
    "# print(classification_report(Y_validation, predictions))\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((regr.predict(X_validation) - Y_validation) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(X_validation, Y_validation))\n",
    "\n",
    "y_pred = regr.predict(X_validation)\n",
    "# print y_pred\n",
    "# print Y_validation\n",
    "# print accuracy_score(Y_validation, y_pred)\n",
    "\n",
    "# cv = KFold(len(X_train), 10, shuffle=True, random_state=33)\n",
    "\n",
    "# #decf = LinearRegression.decision_function(train, target)\n",
    "# test = LinearRegression.predict(X_train, Y_train)\n",
    "# score = cross_val_score(regr,X_train, Y_train,cv=cv )\n",
    "\n",
    "# print(\"Score: {}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "n_samples, n_features = 10, 5\n",
    "clf = KernelRidge(alpha=1.0)\n",
    "clf.fit(X_train, Y_train) \n",
    "# KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel='linear',\n",
    "#             kernel_params=None)\n",
    "regr_acc(X_validation, Y_validation, clf)\n",
    "# y_kr = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## random forest\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, Y_train) \n",
    "prediction = model.predict(X_validation)\n",
    "# print prediction\n",
    "print mean_squared_error(Y_validation, prediction)\n",
    "regr_acc(X_validation, Y_validation, model)\n",
    "\n",
    "# model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(model, X_train, Y_train).mean()\n",
    "print score\n",
    "# regr_acc(X_validation, Y_validation, model)\n",
    "\n",
    "\n",
    "## from article\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# model = RandomForestClassifier(n_estimators=1000) \n",
    "# model.fit(X_train, Y_train) \n",
    "# disbursed = model.predict_proba(X_validation) \n",
    "# # fpr, tpr, _ = roc_curve(Y_validation, disbursed[:,1]) \n",
    "# # roc_auc = auc(fpr, tpr) \n",
    "# # print roc_auc\n",
    "# predictions = model.predict(X_validation)\n",
    "# print(accuracy_score(Y_validation, predictions))\n",
    "# ## row is the thing it is actually in, in this case lots of things are thought of as 4\n",
    "# print(confusion_matrix(Y_validation, predictions))\n",
    "# print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importances = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "#              axis=0)\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# print feature_list\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(X.shape[1]):\n",
    "#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(X.shape[1]), importances[indices],\n",
    "#        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(X.shape[1]), indices)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"Mean squared error: %.2f\"\n",
    "#       % np.mean((y_kr - Y_validation) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "# print('Variance score: %.2f' % clf.score(X_validation, Y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(X_validation), len(Y_validation)\n",
    "# # Plot outputs\n",
    "plt.scatter(predictions, Y_validation,  color='black')\n",
    "plt.ylim(0, 100)\n",
    "# plt.plot(Y_validation, predictions, color='blue',\n",
    "#          linewidth=3)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# plt.show()\n",
    "# # print cv_results\n",
    "print max(Y_train), min(Y_train), np.mean(Y_train), np.median(Y_train), np.std(Y_train)\n",
    "print len(Y_train[Y_train > 250]), len(Y_train)\n",
    "# plt.hist(Y_train, bins=100, range=(0,100))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = model_data_dict[dependent]\n",
    "# print temp.columns.tolist()\n",
    "for i in temp.columns.tolist():\n",
    "    print i\n",
    "    plt.hist(temp[i])\n",
    "    plt.xlabel(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print model_data_dict.keys()\n",
    "# print model_data_dict['L10AM']['LBXIN'].head(90)\n",
    "\n",
    "print len(model_data_dict['L10AM']['LBXIN'])\n",
    "print len(model_data_dict['L10AM'].dropna())\n",
    "# model_data_dict['L10AM'].to_csv('insulin_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## correlations\n",
    "df = model_data_dict['L10AM']\n",
    "x = 'INDFMIN2'\n",
    "y = 'LBXIN'\n",
    "plt.scatter(df[x], df[y])\n",
    "# plt.ylabel(y)\n",
    "# plt.xlabel(x)\n",
    "a = df.corr()\n",
    "# print a\n",
    "b = a[y].dropna()\n",
    "b.sort(ascending=True, kind='quicksort', na_position='last', inplace=False)\n",
    "print b[x] #dr1tsugr + dr1ttfat + dr1tcarb + dr1tprot + dr1tkcal\n",
    "print b\n",
    "\n",
    "# print master.head()\n",
    "# print master['LBXIN_y'].head()\n",
    "# print filt_data_dict.keys()\n",
    "\n",
    "# print filt_data_dict['L10AM_C_2003_2004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## if needed to change NaN to 0\n",
    "# a = filt_data_dict['ACQ_B_2001_2002']\n",
    "# a.fillna(value=0)\n",
    "\n",
    "## proof that seqn are different for all years\n",
    "print min(filt_data_dict['OCQ_1999_2000']['SEQN']), max(filt_data_dict['OCQ_1999_2000']['SEQN'])\n",
    "print min(filt_data_dict['OCQ_B_2001_2002']['SEQN']), max(filt_data_dict['OCQ_B_2001_2002']['SEQN'])\n",
    "print min(filt_data_dict['OCQ_C_2003_2004']['SEQN']), max(filt_data_dict['OCQ_C_2003_2004']['SEQN'])\n",
    "print min(filt_data_dict['OCQ_D_2005_2006']['SEQN']), max(filt_data_dict['OCQ_D_2005_2006']['SEQN'])\n",
    "print min(filt_data_dict['OCQ_E_2007_2008']['SEQN']), max(filt_data_dict['OCQ_E_2007_2008']['SEQN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combinded_var_dict['OCQ'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combinded_var_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = filt_data_dict['DEMO_F_2009_2010']['DMDEDUC3']\n",
    "print a.describe()\n",
    "a = a.fillna(0)\n",
    "a = list(a)\n",
    "print {x:a.count(x) for x in a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_dict['OCD150']\n",
    "# 0.284399062605 DMDHHSZA 5974 24963 0.239314184994\n",
    "# 0.815701372615 DMDHHSZB 5974 24963 0.239314184994\n",
    "# 0.52109139605 DMDHHSZE 5974 24963 0.239314184994\n",
    "# 3.33677031745 DMDHSEDU 12317 24963 0.493410247166\n",
    "# 7.76969215842 INDFMIN2 11337 24963 0.454152145175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
